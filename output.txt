The m-a-p model family is designed to understand music by generating embeddings based on the training data and pre-training paradigm used. The models can be used for various downstream tasks such as music generation, music information retrieval (MIR), or music classification.

To use these embeddings for generating new music, you would need to:

1. Extract embeddings from the pre-trained model using an existing piece of music as input.
2. Use the extracted embeddings as features for training a generative model like GANs, VAEs, or RNNs/LSTMs.
3. Generate new pieces of music using the trained generative model.

With the MERT-v1 model, which uses pseudo labels from 8 codebooks from encodec, you could also use it directly for music generation by predicting the next token in an MLM fashion and reconstructing the audio from the predicted codebook entry.

Overall, these embeddings act as a basis to create new music by learning patterns and structures within the input data. By using these high-quality embeddings, generative models can produce better music outputs that follow the patterns of the original training data.